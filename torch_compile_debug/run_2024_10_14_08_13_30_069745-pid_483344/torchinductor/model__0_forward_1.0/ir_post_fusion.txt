buf0: SchedulerNode(ComputedBuffer)
buf0.writes = [MemoryDep('buf0', c0, {c0: 30720}, None)]
buf0.unmet_dependencies = []
buf0.met_dependencies = [MemoryDep('primals_9', c0, {c0: 30720}, None)]
buf0.users = [NodeUser(node=SchedulerNode(name='buf1'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf2'), can_inplace=False, is_weak=False), NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
buf0.group.device = cuda:0
buf0.group.iteration = (30720, 1)
buf0.sizes = ([30720], [])
primals_9_layout = FixedLayout('cuda', torch.float16, size=[10, 3072], stride=[3072, 1])
buf0_layout = FixedLayout('cuda', torch.float16, size=[10, 3072], stride=[3072, 1])
class buf0_loop_body:
    var_ranges = {z0: 30720}
    index0 = z0
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('primals_9', get_index)
        to_dtype = ops.to_dtype(load, torch.float32, src_dtype = torch.float16)
        get_index_1 = self.get_index('index0')
        load_1 = ops.load('primals_9', get_index_1)
        to_dtype_1 = ops.to_dtype(load_1, torch.float32, src_dtype = torch.float16)
        sigmoid = ops.sigmoid(to_dtype_1)
        mul = ops.mul(to_dtype, sigmoid)
        to_dtype_2 = ops.to_dtype(mul, torch.float16, src_dtype = torch.float32)
        get_index_2 = self.get_index('index0')
        store = ops.store('buf0', get_index_2, to_dtype_2, None)
        return store
buf0 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[32768], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 1, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 30720
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x0 = xindex
        tmp0 = tl.load(in_ptr0 + (x0), None).to(tl.float32)
        tmp1 = tmp0.to(tl.float32)
        tmp2 = tl.sigmoid(tmp1)
        tmp3 = tmp1 * tmp2
        tmp4 = tmp3.to(tl.float32)
        tl.store(out_ptr0 + (x0), tmp4, None)


buf1: SchedulerNode(MultiTemplateBuffer)
buf1.writes = [MemoryDep('buf1', c0, {c0: 184320}, None)]
buf1.unmet_dependencies = [StarDep(name='buf0', mode=None)]
buf1.met_dependencies = [StarDep(name='primals_1', mode=None)]
buf1.users = [NodeUser(node=SchedulerNode(name='buf3'), can_inplace=False, is_weak=False)]
buf1.group.device = cuda:0
buf1.group.iteration = (184320, 1)
buf1.sizes = ([10, 18432], ())
buf0_layout = FixedLayout('cuda', torch.float16, size=[10, 3072], stride=[3072, 1])
primals_1_layout = FixedLayout('cuda', torch.float16, size=[18432, 3072], stride=[3072, 1])
buf1_layout = FixedLayout('cuda', torch.float16, size=[10, 18432], stride=[18432, 1])
buf1 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.template(
        num_stages=3,
        num_warps=4,
        triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    )
    @triton.jit
    def triton_(arg_A, arg_B, out_ptr0):
        GROUP_M : tl.constexpr = 8
        EVEN_K : tl.constexpr = True
        ALLOW_TF32 : tl.constexpr = False
        ACC_TYPE : tl.constexpr = tl.float32
        B_PROLOGUE_CAST_TYPE : tl.constexpr = None
        BLOCK_M : tl.constexpr = 16
        BLOCK_N : tl.constexpr = 64
        BLOCK_K : tl.constexpr = 64
        A = arg_A
        B = arg_B

        M = 10
        N = 18432
        K = 3072
        if M * N == 0:
            # early exit due to zero-size input(s)
            return
        stride_am = 3072
        stride_ak = 1
        stride_bk = 1
        stride_bn = 3072

        # based on triton.ops.matmul
        pid = tl.program_id(0)
        grid_m = (M + BLOCK_M - 1) // BLOCK_M
        grid_n = (N + BLOCK_N - 1) // BLOCK_N

        # re-order program ID for better L2 performance
        width = GROUP_M * grid_n
        group_id = pid // width
        group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
        pid_m = group_id * GROUP_M + (pid % group_size)
        pid_n = (pid % width) // (group_size)

        rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
        rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
        if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
            ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
        else:
            ram = rm % M
        if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
            rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
        else:
            rbn = rn % N
        rk = tl.arange(0, BLOCK_K)
        A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
        B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
        for k in range(K, 0, -BLOCK_K):
            if EVEN_K:
                a = tl.load(A)
                b = tl.load(B)
            else:
                a = tl.load(A, mask=rk[None, :] < k, other=0.)
                b = tl.load(B, mask=rk[:, None] < k, other=0.)
            if B_PROLOGUE_CAST_TYPE is not None:
                b = b.to(B_PROLOGUE_CAST_TYPE)
            acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
            A += BLOCK_K * stride_ak
            B += BLOCK_K * stride_bk

        # rematerialize rm and rn to save registers
        rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
        rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
        idx_m = rm[:, None]
        idx_n = rn[None, :]
        mask = (idx_m < M) & (idx_n < N)

        # inductor generates a suffix
        xindex = idx_n + (18432*idx_m)
        tl.store(out_ptr0 + (tl.broadcast_to(xindex, mask.shape)), acc, mask)


buf2: SchedulerNode(MultiTemplateBuffer)
buf2.writes = [MemoryDep('buf2', c0, {c0: 184320}, None)]
buf2.unmet_dependencies = [StarDep(name='buf0', mode=None)]
buf2.met_dependencies = [StarDep(name='primals_3', mode=None)]
buf2.users = [NodeUser(node=SchedulerNode(name='buf5'), can_inplace=False, is_weak=False)]
buf2.group.device = cuda:0
buf2.group.iteration = (184320, 1)
buf2.sizes = ([10, 18432], ())
buf0_layout = FixedLayout('cuda', torch.float16, size=[10, 3072], stride=[3072, 1])
primals_3_layout = FixedLayout('cuda', torch.float16, size=[18432, 3072], stride=[3072, 1])
buf2_layout = FixedLayout('cuda', torch.float16, size=[10, 18432], stride=[18432, 1])
buf2 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.template(
        num_stages=3,
        num_warps=4,
        triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2), equal_to_1=())]},
        inductor_meta={'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    )
    @triton.jit
    def triton_(arg_A, arg_B, out_ptr0):
        GROUP_M : tl.constexpr = 8
        EVEN_K : tl.constexpr = True
        ALLOW_TF32 : tl.constexpr = False
        ACC_TYPE : tl.constexpr = tl.float32
        B_PROLOGUE_CAST_TYPE : tl.constexpr = None
        BLOCK_M : tl.constexpr = 16
        BLOCK_N : tl.constexpr = 64
        BLOCK_K : tl.constexpr = 64
        A = arg_A
        B = arg_B

        M = 10
        N = 18432
        K = 3072
        if M * N == 0:
            # early exit due to zero-size input(s)
            return
        stride_am = 3072
        stride_ak = 1
        stride_bk = 1
        stride_bn = 3072

        # based on triton.ops.matmul
        pid = tl.program_id(0)
        grid_m = (M + BLOCK_M - 1) // BLOCK_M
        grid_n = (N + BLOCK_N - 1) // BLOCK_N

        # re-order program ID for better L2 performance
        width = GROUP_M * grid_n
        group_id = pid // width
        group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
        pid_m = group_id * GROUP_M + (pid % group_size)
        pid_n = (pid % width) // (group_size)

        rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
        rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
        if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
            ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
        else:
            ram = rm % M
        if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
            rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
        else:
            rbn = rn % N
        rk = tl.arange(0, BLOCK_K)
        A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
        B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
        for k in range(K, 0, -BLOCK_K):
            if EVEN_K:
                a = tl.load(A)
                b = tl.load(B)
            else:
                a = tl.load(A, mask=rk[None, :] < k, other=0.)
                b = tl.load(B, mask=rk[:, None] < k, other=0.)
            if B_PROLOGUE_CAST_TYPE is not None:
                b = b.to(B_PROLOGUE_CAST_TYPE)
            acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
            A += BLOCK_K * stride_ak
            B += BLOCK_K * stride_bk

        # rematerialize rm and rn to save registers
        rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
        rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
        idx_m = rm[:, None]
        idx_n = rn[None, :]
        mask = (idx_m < M) & (idx_n < N)

        # inductor generates a suffix
        xindex = idx_n + (18432*idx_m)
        tl.store(out_ptr0 + (tl.broadcast_to(xindex, mask.shape)), acc, mask)


buf3: SchedulerNode(ComputedBuffer)
buf3.writes = [MemoryDep('buf3', c0, {c0: 125337600}, None)]
buf3.unmet_dependencies = 
    [   MemoryDep('buf1', 18432*c0 + c2 + 3072, {c0: 10, c1: 4080, c2: 3072}, None),
        MemoryDep('buf1', 18432*c0 + c2, {c0: 10, c1: 4080, c2: 3072}, None)]
buf3.met_dependencies = 
    [   MemoryDep('primals_10', c0, {c0: 125337600}, None),
        MemoryDep('primals_2', c1 + 3072, {c0: 40800, c1: 3072}, None),
        MemoryDep('primals_2', c1, {c0: 40800, c1: 3072}, None)]
buf3.users = [NodeUser(node=SchedulerNode(name='buf4'), can_inplace=False, is_weak=False), NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
buf3.group.device = cuda:0
buf3.group.iteration = (125337600, 1)
buf3.sizes = ([10, 4080, 3072], [])
primals_2_layout = FixedLayout('cuda', torch.float16, size=[18432], stride=[1])
primals_10_layout = FixedLayout('cuda', torch.float16, size=[10, 4080, 3072], stride=[12533760, 3072, 1])
buf1_layout = FixedLayout('cuda', torch.float16, size=[10, 18432], stride=[18432, 1])
primals_2_layout = FixedLayout('cuda', torch.float16, size=[18432], stride=[1])
buf1_layout = FixedLayout('cuda', torch.float16, size=[10, 18432], stride=[18432, 1])
buf3_layout = FixedLayout('cuda', torch.float16, size=[10, 4080, 3072], stride=[12533760, 3072, 1])
class buf3_loop_body:
    var_ranges = {z0: 10, z1: 4080, z2: 3072}
    index0 = 18432*z0 + z2 + 3072
    index1 = z2 + 3072
    index2 = 12533760*z0 + 3072*z1 + z2
    index3 = 18432*z0 + z2
    index4 = z2
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf1', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('primals_2', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(1.0, torch.float16)
        add_1 = ops.add(add, constant)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('primals_10', get_index_2)
        mul = ops.mul(add_1, load_2)
        get_index_3 = self.get_index('index3')
        load_3 = ops.load('buf1', get_index_3)
        get_index_4 = self.get_index('index4')
        load_4 = ops.load('primals_2', get_index_4)
        add_2 = ops.add(load_3, load_4)
        add_3 = ops.add(mul, add_2)
        get_index_5 = self.get_index('index2')
        store = ops.store('buf3', get_index_5, add_3, None)
        return store
buf3 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[134217728], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 125337600
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x0 = xindex % 3072
        x2 = (xindex // 12533760)
        x4 = xindex
        tmp0 = tl.load(in_ptr0 + (3072 + x0 + (18432*x2)), None, eviction_policy='evict_last').to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (3072 + x0), None, eviction_policy='evict_last').to(tl.float32)
        tmp5 = tl.load(in_ptr2 + (x4), None).to(tl.float32)
        tmp7 = tl.load(in_ptr0 + (x0 + (18432*x2)), None, eviction_policy='evict_last').to(tl.float32)
        tmp8 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last').to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp3 = 1.0
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 * tmp5
        tmp9 = tmp7 + tmp8
        tmp10 = tmp6 + tmp9
        tl.store(out_ptr0 + (x4), tmp10, None)


buf4: SchedulerNode(MultiTemplateBuffer)
buf4.writes = [MemoryDep('buf4', c0, {c0: 376012800}, None)]
buf4.unmet_dependencies = [StarDep(name='buf3', mode=None)]
buf4.met_dependencies = [StarDep(name='primals_5', mode=None), StarDep(name='primals_6', mode=None)]
buf4.users = [NodeUser(node=SchedulerNode(name='buf7'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf8'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf9'), can_inplace=False, is_weak=False)]
buf4.group.device = cuda:0
buf4.group.iteration = (376012800, 1)
buf4.sizes = ([40800, 9216], ())
primals_6_layout = FixedLayout('cuda', torch.float16, size=[9216], stride=[1])
buf3_layout = FixedLayout('cuda', torch.float16, size=[10, 4080, 3072], stride=[12533760, 3072, 1])
primals_5_layout = FixedLayout('cuda', torch.float16, size=[9216, 3072], stride=[3072, 1])
buf4_layout = FixedLayout('cuda', torch.float16, size=[40800, 9216], stride=[9216, 1])
buf4 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.template(
        num_stages=5,
        num_warps=8,
        triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
        inductor_meta={'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    )
    @triton.jit
    def triton_(in_ptr0, arg_A, arg_B, out_ptr0):
        GROUP_M : tl.constexpr = 8
        EVEN_K : tl.constexpr = True
        ALLOW_TF32 : tl.constexpr = False
        ACC_TYPE : tl.constexpr = tl.float32
        B_PROLOGUE_CAST_TYPE : tl.constexpr = None
        BLOCK_M : tl.constexpr = 128
        BLOCK_N : tl.constexpr = 128
        BLOCK_K : tl.constexpr = 64
        A = arg_A
        B = arg_B

        M = 40800
        N = 9216
        K = 3072
        if M * N == 0:
            # early exit due to zero-size input(s)
            return
        stride_am = 3072
        stride_ak = 1
        stride_bk = 1
        stride_bn = 3072

        # based on triton.ops.matmul
        pid = tl.program_id(0)
        grid_m = (M + BLOCK_M - 1) // BLOCK_M
        grid_n = (N + BLOCK_N - 1) // BLOCK_N

        # re-order program ID for better L2 performance
        width = GROUP_M * grid_n
        group_id = pid // width
        group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
        pid_m = group_id * GROUP_M + (pid % group_size)
        pid_n = (pid % width) // (group_size)

        rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
        rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
        if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
            ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
        else:
            ram = rm % M
        if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
            rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
        else:
            rbn = rn % N
        rk = tl.arange(0, BLOCK_K)
        A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
        B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
        for k in range(K, 0, -BLOCK_K):
            if EVEN_K:
                a = tl.load(A)
                b = tl.load(B)
            else:
                a = tl.load(A, mask=rk[None, :] < k, other=0.)
                b = tl.load(B, mask=rk[:, None] < k, other=0.)
            if B_PROLOGUE_CAST_TYPE is not None:
                b = b.to(B_PROLOGUE_CAST_TYPE)
            acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
            A += BLOCK_K * stride_ak
            B += BLOCK_K * stride_bk

        # rematerialize rm and rn to save registers
        rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
        rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
        idx_m = rm[:, None]
        idx_n = rn[None, :]
        mask = (idx_m < M) & (idx_n < N)

        # inductor generates a suffix
        xindex = idx_n + (9216*idx_m)
        tmp0 = tl.load(in_ptr0 + (tl.broadcast_to(idx_n, mask.shape)), mask, eviction_policy='evict_last').to(tl.float32)
        tmp1 = acc + tmp0
        tl.store(out_ptr0 + (tl.broadcast_to(xindex, mask.shape)), tmp1, mask)


buf5: SchedulerNode(ComputedBuffer)
buf5.writes = [MemoryDep('buf5', c0, {c0: 7864320}, None)]
buf5.unmet_dependencies = 
    [   MemoryDep('buf2', 18432*c0 + c2 + 3072, {c0: 10, c1: 256, c2: 3072}, None),
        MemoryDep('buf2', 18432*c0 + c2, {c0: 10, c1: 256, c2: 3072}, None)]
buf5.met_dependencies = 
    [   MemoryDep('primals_11', c0, {c0: 7864320}, None),
        MemoryDep('primals_4', c1 + 3072, {c0: 2560, c1: 3072}, None),
        MemoryDep('primals_4', c1, {c0: 2560, c1: 3072}, None)]
buf5.users = [NodeUser(node=SchedulerNode(name='buf6'), can_inplace=False, is_weak=False), NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
buf5.group.device = cuda:0
buf5.group.iteration = (7864320, 1)
buf5.sizes = ([10, 256, 3072], [])
buf2_layout = FixedLayout('cuda', torch.float16, size=[10, 18432], stride=[18432, 1])
primals_11_layout = FixedLayout('cuda', torch.float16, size=[10, 256, 3072], stride=[786432, 3072, 1])
primals_4_layout = FixedLayout('cuda', torch.float16, size=[18432], stride=[1])
buf2_layout = FixedLayout('cuda', torch.float16, size=[10, 18432], stride=[18432, 1])
primals_4_layout = FixedLayout('cuda', torch.float16, size=[18432], stride=[1])
buf5_layout = FixedLayout('cuda', torch.float16, size=[10, 256, 3072], stride=[786432, 3072, 1])
class buf5_loop_body:
    var_ranges = {z0: 10, z1: 256, z2: 3072}
    index0 = 18432*z0 + z2 + 3072
    index1 = z2 + 3072
    index2 = 786432*z0 + 3072*z1 + z2
    index3 = 18432*z0 + z2
    index4 = z2
    def body(self, ops):
        get_index = self.get_index('index0')
        load = ops.load('buf2', get_index)
        get_index_1 = self.get_index('index1')
        load_1 = ops.load('primals_4', get_index_1)
        add = ops.add(load, load_1)
        constant = ops.constant(1.0, torch.float16)
        add_1 = ops.add(add, constant)
        get_index_2 = self.get_index('index2')
        load_2 = ops.load('primals_11', get_index_2)
        mul = ops.mul(add_1, load_2)
        get_index_3 = self.get_index('index3')
        load_3 = ops.load('buf2', get_index_3)
        get_index_4 = self.get_index('index4')
        load_4 = ops.load('primals_4', get_index_4)
        add_2 = ops.add(load_3, load_4)
        add_3 = ops.add(mul, add_2)
        get_index_5 = self.get_index('index2')
        store = ops.store('buf5', get_index_5, add_3, None)
        return store
buf5 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[8388608], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16', 4: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 5, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 7864320
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x0 = xindex % 3072
        x2 = (xindex // 786432)
        x4 = xindex
        tmp0 = tl.load(in_ptr0 + (3072 + x0 + (18432*x2)), None, eviction_policy='evict_last').to(tl.float32)
        tmp1 = tl.load(in_ptr1 + (3072 + x0), None, eviction_policy='evict_last').to(tl.float32)
        tmp5 = tl.load(in_ptr2 + (x4), None).to(tl.float32)
        tmp7 = tl.load(in_ptr0 + (x0 + (18432*x2)), None, eviction_policy='evict_last').to(tl.float32)
        tmp8 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last').to(tl.float32)
        tmp2 = tmp0 + tmp1
        tmp3 = 1.0
        tmp4 = tmp2 + tmp3
        tmp6 = tmp4 * tmp5
        tmp9 = tmp7 + tmp8
        tmp10 = tmp6 + tmp9
        tl.store(out_ptr0 + (x4), tmp10, None)


buf6: SchedulerNode(MultiTemplateBuffer)
buf6.writes = [MemoryDep('buf6', c0, {c0: 23592960}, None)]
buf6.unmet_dependencies = [StarDep(name='buf5', mode=None)]
buf6.met_dependencies = [StarDep(name='primals_7', mode=None), StarDep(name='primals_8', mode=None)]
buf6.users = [NodeUser(node=SchedulerNode(name='buf7'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf8'), can_inplace=False, is_weak=False), NodeUser(node=SchedulerNode(name='buf9'), can_inplace=False, is_weak=False)]
buf6.group.device = cuda:0
buf6.group.iteration = (23592960, 1)
buf6.sizes = ([2560, 9216], ())
buf5_layout = FixedLayout('cuda', torch.float16, size=[10, 256, 3072], stride=[786432, 3072, 1])
primals_7_layout = FixedLayout('cuda', torch.float16, size=[9216, 3072], stride=[3072, 1])
primals_8_layout = FixedLayout('cuda', torch.float16, size=[9216], stride=[1])
buf6_layout = FixedLayout('cuda', torch.float16, size=[2560, 9216], stride=[9216, 1])
buf6 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.template(
        num_stages=3,
        num_warps=4,
        triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: '*fp16'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
        inductor_meta={'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
    )
    @triton.jit
    def triton_(in_ptr0, arg_A, arg_B, out_ptr0):
        GROUP_M : tl.constexpr = 8
        EVEN_K : tl.constexpr = True
        ALLOW_TF32 : tl.constexpr = False
        ACC_TYPE : tl.constexpr = tl.float32
        B_PROLOGUE_CAST_TYPE : tl.constexpr = None
        BLOCK_M : tl.constexpr = 128
        BLOCK_N : tl.constexpr = 128
        BLOCK_K : tl.constexpr = 64
        A = arg_A
        B = arg_B

        M = 2560
        N = 9216
        K = 3072
        if M * N == 0:
            # early exit due to zero-size input(s)
            return
        stride_am = 3072
        stride_ak = 1
        stride_bk = 1
        stride_bn = 3072

        # based on triton.ops.matmul
        pid = tl.program_id(0)
        grid_m = (M + BLOCK_M - 1) // BLOCK_M
        grid_n = (N + BLOCK_N - 1) // BLOCK_N

        # re-order program ID for better L2 performance
        width = GROUP_M * grid_n
        group_id = pid // width
        group_size = min(grid_m - group_id * GROUP_M, GROUP_M)
        pid_m = group_id * GROUP_M + (pid % group_size)
        pid_n = (pid % width) // (group_size)

        rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
        rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
        if (stride_am == 1 and stride_ak == M) or (stride_am == K and stride_ak == 1):
            ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)
        else:
            ram = rm % M
        if (stride_bk == 1 and stride_bn == K) or (stride_bk == N and stride_bn == 1):
            rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)
        else:
            rbn = rn % N
        rk = tl.arange(0, BLOCK_K)
        A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)
        B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)

        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)
        for k in range(K, 0, -BLOCK_K):
            if EVEN_K:
                a = tl.load(A)
                b = tl.load(B)
            else:
                a = tl.load(A, mask=rk[None, :] < k, other=0.)
                b = tl.load(B, mask=rk[:, None] < k, other=0.)
            if B_PROLOGUE_CAST_TYPE is not None:
                b = b.to(B_PROLOGUE_CAST_TYPE)
            acc += tl.dot(a, b, allow_tf32=ALLOW_TF32)
            A += BLOCK_K * stride_ak
            B += BLOCK_K * stride_bk

        # rematerialize rm and rn to save registers
        rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)
        rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)
        idx_m = rm[:, None]
        idx_n = rn[None, :]
        mask = (idx_m < M) & (idx_n < N)

        # inductor generates a suffix
        xindex = idx_n + (9216*idx_m)
        tmp0 = tl.load(in_ptr0 + (tl.broadcast_to(idx_n, mask.shape)), mask, eviction_policy='evict_last').to(tl.float32)
        tmp1 = acc + tmp0
        tl.store(out_ptr0 + (tl.broadcast_to(xindex, mask.shape)), tmp1, mask)


buf7: SchedulerNode(ComputedBuffer)
buf7.writes = [   MemoryDep('buf7', 13320192*c0 + 128*c1 + 555008*c2 + c3, {c0: 10, c1: 4336, c2: 24, c3: 128}, None)]
buf7.unmet_dependencies = 
    [   MemoryDep('buf4', 37601280*c0 + 9216*c1 + c2 - 2359296, {c0: 10, c1: 4336, c2: 3072}, None),
        MemoryDep('buf6', 2359296*c0 + 9216*c1 + c2, {c0: 10, c1: 4336, c2: 3072}, None)]
buf7.met_dependencies = []
buf7.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
buf7.group.device = cuda:0
buf7.group.iteration = (133201920, 1)
buf7.sizes = ([10, 4336, 24, 128], [])
buf4_layout = FixedLayout('cuda', torch.float16, size=[40800, 9216], stride=[9216, 1])
buf6_layout = FixedLayout('cuda', torch.float16, size=[2560, 9216], stride=[9216, 1])
buf7_layout = FixedLayout('cuda', torch.float16, size=[10, 24, 4336, 128], stride=[13320192, 555008, 128, 1])
class buf7_loop_body:
    var_ranges = {z0: 10, z1: 4336, z2: 24, z3: 128}
    index0 = z1
    index1 = 2359296*z0 + 9216*z1 + 128*z2 + z3
    index2 = 37601280*z0 + 9216*z1 + 128*z2 + z3 - 2359296
    index3 = 13320192*z0 + 128*z1 + 555008*z2 + z3
    def body(self, ops):
        get_index = self.get_index('index0')
        index_expr = ops.index_expr(get_index, torch.int64)
        constant = ops.constant(0, torch.int64)
        ge = ops.ge(index_expr, constant)
        get_index_1 = self.get_index('index0')
        index_expr_1 = ops.index_expr(get_index_1, torch.int64)
        constant_1 = ops.constant(256, torch.int64)
        lt = ops.lt(index_expr_1, constant_1)
        masked_subblock1 = self.masked_subblock1(lt, 0.0)
        get_index_2 = self.get_index('index0')
        index_expr_2 = ops.index_expr(get_index_2, torch.int64)
        constant_2 = ops.constant(256, torch.int64)
        ge_1 = ops.ge(index_expr_2, constant_2)
        get_index_3 = self.get_index('index0')
        index_expr_3 = ops.index_expr(get_index_3, torch.int64)
        constant_3 = ops.constant(4336, torch.int64)
        lt_1 = ops.lt(index_expr_3, constant_3)
        masked_subblock2 = self.masked_subblock2(ge_1, 0.0)
        where = ops.where(lt, masked_subblock1, masked_subblock2)
        get_index_4 = self.get_index('index3')
        store = ops.store('buf7', get_index_4, where, None)
        return store
    def masked_subblock1(self, ops):
        get_index = self.get_index('index1')
        load = ops.load('buf6', get_index)
        return load
    def masked_subblock2(self, ops):
        get_index = self.get_index('index2')
        load = ops.load('buf4', get_index)
        return load
buf7 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[134217728], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 133201920
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = (xindex // 3072) % 4336
        x3 = (xindex // 13320192)
        x4 = xindex % 3072
        x0 = xindex % 128
        x1 = (xindex // 128) % 24
        tmp0 = x2
        tmp1 = tl.full([1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1], 256, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (x4 + (9216*x2) + (2359296*x3)), tmp4, other=0.0).to(tl.float32)
        tmp6 = tl.full(tmp5.shape, 0.0, tmp5.dtype)
        tmp7 = tl.where(tmp4, tmp5, tmp6)
        tmp8 = tmp0 >= tmp3
        tmp9 = tl.full([1], 4336, tl.int64)
        tmp10 = tmp0 < tmp9
        tmp11 = tl.load(in_ptr1 + ((-2359296) + x4 + (9216*x2) + (37601280*x3)), tmp8, other=0.0).to(tl.float32)
        tmp12 = tl.full(tmp11.shape, 0.0, tmp11.dtype)
        tmp13 = tl.where(tmp8, tmp11, tmp12)
        tmp14 = tl.where(tmp4, tmp7, tmp13)
        tl.store(out_ptr0 + (x0 + (128*x2) + (555008*x1) + (13320192*x3)), tmp14, None)


buf8: SchedulerNode(ComputedBuffer)
buf8.writes = [   MemoryDep('buf8', 13320192*c0 + 128*c1 + 555008*c2 + c3, {c0: 10, c1: 4336, c2: 24, c3: 128}, None)]
buf8.unmet_dependencies = 
    [   MemoryDep('buf4', 37601280*c0 + 9216*c1 + c2 - 2356224, {c0: 10, c1: 4336, c2: 3072}, None),
        MemoryDep('buf6', 2359296*c0 + 9216*c1 + c2 + 3072, {c0: 10, c1: 4336, c2: 3072}, None)]
buf8.met_dependencies = []
buf8.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
buf8.group.device = cuda:0
buf8.group.iteration = (133201920, 1)
buf8.sizes = ([10, 4336, 24, 128], [])
buf4_layout = FixedLayout('cuda', torch.float16, size=[40800, 9216], stride=[9216, 1])
buf6_layout = FixedLayout('cuda', torch.float16, size=[2560, 9216], stride=[9216, 1])
buf8_layout = FixedLayout('cuda', torch.float16, size=[10, 24, 4336, 128], stride=[13320192, 555008, 128, 1])
class buf8_loop_body:
    var_ranges = {z0: 10, z1: 4336, z2: 24, z3: 128}
    index0 = z1
    index1 = 2359296*z0 + 9216*z1 + 128*z2 + z3 + 3072
    index2 = 37601280*z0 + 9216*z1 + 128*z2 + z3 - 2356224
    index3 = 13320192*z0 + 128*z1 + 555008*z2 + z3
    def body(self, ops):
        get_index = self.get_index('index0')
        index_expr = ops.index_expr(get_index, torch.int64)
        constant = ops.constant(0, torch.int64)
        ge = ops.ge(index_expr, constant)
        get_index_1 = self.get_index('index0')
        index_expr_1 = ops.index_expr(get_index_1, torch.int64)
        constant_1 = ops.constant(256, torch.int64)
        lt = ops.lt(index_expr_1, constant_1)
        masked_subblock1 = self.masked_subblock1(lt, 0.0)
        get_index_2 = self.get_index('index0')
        index_expr_2 = ops.index_expr(get_index_2, torch.int64)
        constant_2 = ops.constant(256, torch.int64)
        ge_1 = ops.ge(index_expr_2, constant_2)
        get_index_3 = self.get_index('index0')
        index_expr_3 = ops.index_expr(get_index_3, torch.int64)
        constant_3 = ops.constant(4336, torch.int64)
        lt_1 = ops.lt(index_expr_3, constant_3)
        masked_subblock2 = self.masked_subblock2(ge_1, 0.0)
        where = ops.where(lt, masked_subblock1, masked_subblock2)
        get_index_4 = self.get_index('index3')
        store = ops.store('buf8', get_index_4, where, None)
        return store
    def masked_subblock1(self, ops):
        get_index = self.get_index('index1')
        load = ops.load('buf6', get_index)
        return load
    def masked_subblock2(self, ops):
        get_index = self.get_index('index2')
        load = ops.load('buf4', get_index)
        return load
buf8 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[134217728], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 133201920
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = (xindex // 3072) % 4336
        x3 = (xindex // 13320192)
        x4 = xindex % 3072
        x0 = xindex % 128
        x1 = (xindex // 128) % 24
        tmp0 = x2
        tmp1 = tl.full([1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1], 256, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (3072 + x4 + (9216*x2) + (2359296*x3)), tmp4, other=0.0).to(tl.float32)
        tmp6 = tl.full(tmp5.shape, 0.0, tmp5.dtype)
        tmp7 = tl.where(tmp4, tmp5, tmp6)
        tmp8 = tmp0 >= tmp3
        tmp9 = tl.full([1], 4336, tl.int64)
        tmp10 = tmp0 < tmp9
        tmp11 = tl.load(in_ptr1 + ((-2356224) + x4 + (9216*x2) + (37601280*x3)), tmp8, other=0.0).to(tl.float32)
        tmp12 = tl.full(tmp11.shape, 0.0, tmp11.dtype)
        tmp13 = tl.where(tmp8, tmp11, tmp12)
        tmp14 = tl.where(tmp4, tmp7, tmp13)
        tl.store(out_ptr0 + (x0 + (128*x2) + (555008*x1) + (13320192*x3)), tmp14, None)


buf9: SchedulerNode(ComputedBuffer)
buf9.writes = [   MemoryDep('buf9', 13320192*c0 + 128*c1 + 555008*c2 + c3, {c0: 10, c1: 4336, c2: 24, c3: 128}, None)]
buf9.unmet_dependencies = 
    [   MemoryDep('buf4', 37601280*c0 + 9216*c1 + c2 - 2353152, {c0: 10, c1: 4336, c2: 3072}, None),
        MemoryDep('buf6', 2359296*c0 + 9216*c1 + c2 + 6144, {c0: 10, c1: 4336, c2: 3072}, None)]
buf9.met_dependencies = []
buf9.users = [NodeUser(node=OUTPUT, can_inplace=False, is_weak=False)]
buf9.group.device = cuda:0
buf9.group.iteration = (133201920, 1)
buf9.sizes = ([10, 4336, 24, 128], [])
buf4_layout = FixedLayout('cuda', torch.float16, size=[40800, 9216], stride=[9216, 1])
buf6_layout = FixedLayout('cuda', torch.float16, size=[2560, 9216], stride=[9216, 1])
buf9_layout = FixedLayout('cuda', torch.float16, size=[10, 24, 4336, 128], stride=[13320192, 555008, 128, 1])
class buf9_loop_body:
    var_ranges = {z0: 10, z1: 4336, z2: 24, z3: 128}
    index0 = z1
    index1 = 2359296*z0 + 9216*z1 + 128*z2 + z3 + 6144
    index2 = 37601280*z0 + 9216*z1 + 128*z2 + z3 - 2353152
    index3 = 13320192*z0 + 128*z1 + 555008*z2 + z3
    def body(self, ops):
        get_index = self.get_index('index0')
        index_expr = ops.index_expr(get_index, torch.int64)
        constant = ops.constant(0, torch.int64)
        ge = ops.ge(index_expr, constant)
        get_index_1 = self.get_index('index0')
        index_expr_1 = ops.index_expr(get_index_1, torch.int64)
        constant_1 = ops.constant(256, torch.int64)
        lt = ops.lt(index_expr_1, constant_1)
        masked_subblock1 = self.masked_subblock1(lt, 0.0)
        get_index_2 = self.get_index('index0')
        index_expr_2 = ops.index_expr(get_index_2, torch.int64)
        constant_2 = ops.constant(256, torch.int64)
        ge_1 = ops.ge(index_expr_2, constant_2)
        get_index_3 = self.get_index('index0')
        index_expr_3 = ops.index_expr(get_index_3, torch.int64)
        constant_3 = ops.constant(4336, torch.int64)
        lt_1 = ops.lt(index_expr_3, constant_3)
        masked_subblock2 = self.masked_subblock2(ge_1, 0.0)
        where = ops.where(lt, masked_subblock1, masked_subblock2)
        get_index_4 = self.get_index('index3')
        store = ops.store('buf9', get_index_4, where, None)
        return store
    def masked_subblock1(self, ops):
        get_index = self.get_index('index1')
        load = ops.load('buf6', get_index)
        return load
    def masked_subblock2(self, ops):
        get_index = self.get_index('index2')
        load = ops.load('buf4', get_index)
        return load
buf9 Triton code:
    import triton
    import triton.language as tl
    from triton.compiler.compiler import AttrsDescriptor

    from torch._inductor.runtime import triton_helpers, triton_heuristics
    from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
    from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, instance_descriptor, DeviceProperties

    @triton_heuristics.pointwise(
        size_hints=[134217728], 
        filename=__file__,
        triton_meta={'signature': {0: '*fp16', 1: '*fp16', 2: '*fp16', 3: 'i32'}, 'device': DeviceProperties(type='cuda', index=0, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, multi_processor_count=132), 'constants': {}, 'configs': [AttrsDescriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=())]},
        inductor_meta={'autotune_hints': set(), 'kernel_name': 'Placeholder.DESCRIPTIVE_NAME', 'mutated_arg_names': [], 'no_x_dim': False, 'num_load': 2, 'num_reduction': 0, 'backend_hash': '3944FAAD9D18E987535CF029C4441E186640DD453AE5AE6831EDF736C09E0A47', 'are_deterministic_algorithms_enabled': False, 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': False, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': True, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False},
        min_elem_per_thread=0
    )
    @triton.jit
    def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
        xnumel = 133201920
        xoffset = tl.program_id(0) * XBLOCK
        xindex = xoffset + tl.arange(0, XBLOCK)[:]
        xmask = xindex < xnumel
        x2 = (xindex // 3072) % 4336
        x3 = (xindex // 13320192)
        x4 = xindex % 3072
        x0 = xindex % 128
        x1 = (xindex // 128) % 24
        tmp0 = x2
        tmp1 = tl.full([1], 0, tl.int64)
        tmp2 = tmp0 >= tmp1
        tmp3 = tl.full([1], 256, tl.int64)
        tmp4 = tmp0 < tmp3
        tmp5 = tl.load(in_ptr0 + (6144 + x4 + (9216*x2) + (2359296*x3)), tmp4, other=0.0).to(tl.float32)
        tmp6 = tl.full(tmp5.shape, 0.0, tmp5.dtype)
        tmp7 = tl.where(tmp4, tmp5, tmp6)
        tmp8 = tmp0 >= tmp3
        tmp9 = tl.full([1], 4336, tl.int64)
        tmp10 = tmp0 < tmp9
        tmp11 = tl.load(in_ptr1 + ((-2353152) + x4 + (9216*x2) + (37601280*x3)), tmp8, other=0.0).to(tl.float32)
        tmp12 = tl.full(tmp11.shape, 0.0, tmp11.dtype)
        tmp13 = tl.where(tmp8, tmp11, tmp12)
        tmp14 = tl.where(tmp4, tmp7, tmp13)
        tl.store(out_ptr0 + (x0 + (128*x2) + (555008*x1) + (13320192*x3)), tmp14, None)


